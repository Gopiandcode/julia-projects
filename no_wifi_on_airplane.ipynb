{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No-Support-Julia\n",
    "## Introduction\n",
    "There's no wifi on the airplane, but I've got three hours to spare, so I'm just going to be banging my head against the julia stdlib. Let's see what happens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Let's load some data\n",
    "Let's load a basic set of text that I wrote in 5 secs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is some text you will need to parse using Julia.\\nHow can it be parsed? Using the standard library functions.\\nNotice how the lines do not all have the same length, and the values are not consistent.\\nFurthermore, there are some miscellaneous details that are irrelevant to the summarization effort\\nYou will need to extract these as well.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = readstring(open(\"data/test.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some data now, let's try splitting it up first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59-element Array{SubString{String},1}:\n",
       " \"This\"         \n",
       " \"is\"           \n",
       " \"some\"         \n",
       " \"text\"         \n",
       " \"you\"          \n",
       " \"will\"         \n",
       " \"need\"         \n",
       " \"to\"           \n",
       " \"parse\"        \n",
       " \"using\"        \n",
       " \"Julia.\"       \n",
       " \"How\"          \n",
       " \"can\"          \n",
       " ⋮              \n",
       " \"to\"           \n",
       " \"the\"          \n",
       " \"summarization\"\n",
       " \"effort\"       \n",
       " \"You\"          \n",
       " \"will\"         \n",
       " \"need\"         \n",
       " \"to\"           \n",
       " \"extract\"      \n",
       " \"these\"        \n",
       " \"as\"           \n",
       " \"well.\"        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = split(test_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about plotting this stuff in histogram?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's sanitize our words to remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59-element Array{String,1}:\n",
       " \"this\"         \n",
       " \"is\"           \n",
       " \"some\"         \n",
       " \"text\"         \n",
       " \"you\"          \n",
       " \"will\"         \n",
       " \"need\"         \n",
       " \"to\"           \n",
       " \"parse\"        \n",
       " \"using\"        \n",
       " \"julia\"        \n",
       " \"how\"          \n",
       " \"can\"          \n",
       " ⋮              \n",
       " \"to\"           \n",
       " \"the\"          \n",
       " \"summarization\"\n",
       " \"effort\"       \n",
       " \"you\"          \n",
       " \"will\"         \n",
       " \"need\"         \n",
       " \"to\"           \n",
       " \"extract\"      \n",
       " \"these\"        \n",
       " \"as\"           \n",
       " \"well\"         "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanitized = map(a -> lowercase(join(filter(x -> isalnum(x), a))), words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Dict{String, Int}()\n",
    "for word in sanitized\n",
    "    counter[word] = get(counter, word, 0) + 1\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now I'm lost. I've got no idea how to plot this.\n",
    "Let's drop the idea of a histogram and move onto some cooler things then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idmap = Dict{String, Int}()\n",
    "currId = 0\n",
    "for word in sanitized\n",
    "    id = get(idmap, word, -1)\n",
    "    if id == -1\n",
    "        idmap[word] = currId\n",
    "        currId += 1\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can represent each word with unique 44 value vector\n",
    "\n",
    "Hey I've got an idea, let's try word2vec on this.\n",
    "\n",
    "Now I read an article on this, but I just skimmed it - enough to get the big picture, but I'm missing the details.\n",
    "\n",
    "From what I understood from the article, word2vec works by running an autoencoder network against the words, but then the article also mentioned some other stuff they do, like using the past word to predict the current to improve accuracy - I'm not 100% sure, but that sounds about right.\n",
    "\n",
    "Alright. Nice, I've got an idea of what to do. Now. First things first, we need to set up a variational autoencoder.\n",
    "\n",
    "To do this, we'll need a sequence of layers which gradually reduce the input vector of size 44 to some small value representing the hidden layers, then a sequence of layers which gradually increase the size of the vector from the sparse encoding to the initial size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a struct to represent our autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct AutoEncoder\n",
    "    encoder::Array\n",
    "    decoder::Array\n",
    "    input_size::UInt\n",
    "    hidden_layer_size::UInt\n",
    "    layers::UInt\n",
    "    function AutoEncoder(inp, hidd, layers) \n",
    "        step = convert(Int, (hidd - inp)/layers)\n",
    "        encoder = Array{Array{Float64}}(layers)\n",
    "        decoder = Array{Array{Float64}}(layers)\n",
    "        \n",
    "        current_size = inp\n",
    "        for i in 1:layers\n",
    "            encoder[i] = rand(current_size, current_size + step)\n",
    "            current_size += step\n",
    "        end\n",
    "        step *= -1\n",
    "        for i in 1:layers\n",
    "            decoder[i] = rand(current_size, current_size + step)\n",
    "            current_size += step\n",
    "        end\n",
    "        new(encoder, decoder, inp, hidd, layers)\n",
    "    end\n",
    "            \n",
    "        \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we've made  a constructor, which when given the input size, hidden layer size, and layers, auto initializes the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91msyntax: unexpected \"end\"\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91msyntax: unexpected \"end\"\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(Array{Float64,N} where N[[0.379269 0.0453297 … 0.311252 0.986605; 0.305363 0.772192 … 0.644362 0.652224; … ; 0.781224 0.140815 … 0.280981 0.921464; 0.528331 0.948057 … 0.505355 0.545835], [0.491154 0.18796 … 0.256367 0.930093; 0.795206 0.166115 … 0.816345 0.26615; … ; 0.924861 0.738504 … 0.205056 0.213574; 0.237844 0.689262 … 0.717254 0.287205], [0.828815 0.116469 … 0.962458 0.538443; 0.752039 0.433587 … 0.383888 0.824494; … ; 0.123673 0.705926 … 0.608085 0.474822; 0.260323 0.739131 … 0.584931 0.703986], [0.695669 0.704979 … 0.980679 0.170187; 0.870905 0.665081 … 0.134856 0.963135; … ; 0.972461 0.458998 … 0.900291 0.420887; 0.340354 0.694829 … 0.662717 0.46091], [0.21244 0.283266 … 0.157289 0.358607; 0.660753 0.231501 … 0.392178 0.0814015; … ; 0.404754 0.526115 … 0.0947717 0.825928; 0.90473 0.440946 … 0.158078 0.538893]], Array{Float64,N} where N[[0.967083 0.780214 … 0.838402 0.953968; 0.642957 0.655513 … 0.010003 0.872708; … ; 0.159979 0.806286 … 0.26883 0.0445287; 0.478894 0.650159 … 0.481998 0.719939], [0.171651 0.97626 … 0.931134 0.851077; 0.663328 0.0255617 … 0.989366 0.398597; … ; 0.317252 0.733107 … 0.161377 0.918833; 0.275182 0.601359 … 0.858864 0.959802], [0.707484 0.375454 … 0.186164 0.615784; 0.226949 0.889921 … 0.613341 0.983044; … ; 0.465204 0.342789 … 0.619542 0.687761; 0.781966 0.248431 … 0.0559747 0.905347], [0.811423 0.390236 … 0.493437 0.212761; 0.263621 0.607775 … 0.630159 0.38749; … ; 0.928042 0.695532 … 0.578357 0.579245; 0.671947 0.197259 … 0.978356 0.588339], [0.869175 0.181567 … 0.216249 0.775253; 0.515174 0.522901 … 0.844357 0.485321; … ; 0.10356 0.99335 … 0.743824 0.00216504; 0.857771 0.976082 … 0.19901 0.699841]], 0x0000000000000064, 0x000000000000000a, 0x0000000000000005)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = AutoEncoder(100,10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function to unsupervised train the object on a given input\n",
    "\n",
    "first we need an error function - we'll use the euclidean distance for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "euclidean_distance (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function euclidean_distance(a, b)\n",
    "    distance = 0.0\n",
    "    for i in 1:length(a)\n",
    "        a_i = a[i]\n",
    "        b_i = b[i]\n",
    "        distance += (a_i - b_i) * (a_i - b_i)\n",
    "    end\n",
    "    distance\n",
    "end\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance([1;1;1],[1;2;1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the train function - with the default error function being euclid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(p::AutoEncoder, input::Array{Float64}, loss_function=euclidean_distance) \n",
    "    result = input\n",
    "    for i in 1:first(size(p.encoder))\n",
    "        result = p.encoder[i]\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(enc, [1.0,2.0,3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
